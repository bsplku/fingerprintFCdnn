{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries and modules needed in the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# # If you want to sepcify GPU device \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "from modules_indiv_identification import init_data_params\n",
    "from modules_indiv_identification import init_train_params\n",
    "from modules_indiv_identification import load_data\n",
    "from modules_indiv_identification import write_info\n",
    "from modules_indiv_identification import init_pretrained_dnn\n",
    "from modules_indiv_identification import init_dnn_last_layer\n",
    "from modules_indiv_identification import init_cost_optimizer\n",
    "from modules_indiv_identification import init_other_variables\n",
    "from modules_indiv_identification import Hoyers_sparsity_control\n",
    "from modules_indiv_identification import get_err\n",
    "from modules_indiv_identification import plot_save_results\n",
    "\n",
    "start_time = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "@@@@@@@@@@@@@@@@@@@@@@@         Starting         @@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "@@@@@@@@@@    results_20210908_123657_tgHSP_0.80-0.95    @@@@@@@@@@@\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters related to the input data and model architecture\n",
    "directory_open, subject_list, n_subjects, window_length_min, n_samples_run, n_parcels, exclude_parcels, n_units, selected_units, n_hid_layers, n_sp_layers \\\n",
    " = init_data_params(directory_open='./data', subject_list=np.arange(10)+1)\n",
    "\n",
    "# Initialize parameters related to the training and make result directory.\n",
    "final_directory, n_epochs, epoch_step_show, batch_size, lr_init, lr_min, lr_beginanneal, lr_drate, tg_sp_set, beta_max, beta_lr, gamma, layer_activation, optimizer_algorithm, momentum \\\n",
    " = init_train_params(directory_save='./results', n_epochs=200, batch_size=20, lr_init=0.0005, tg_sp_set=[0.8, 0.95], beta_max=[0.0002, 0.001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject 1 loaded\n",
      "subject 2 loaded\n",
      "subject 3 loaded\n",
      "subject 4 loaded\n",
      "subject 5 loaded\n",
      "subject 6 loaded\n",
      "subject 7 loaded\n",
      "subject 8 loaded\n",
      "subject 9 loaded\n",
      "subject 10 loaded\n",
      "--2021-09-08 12:37:13--  https://docs.google.com/uc?export=download&confirm=OOKG&id=1MBrFIxKOW_pxbeAVshmuGp8SKakrMzkw\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.175.78, 2404:6800:4004:808::200e\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.175.78|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0k-as-docs.googleusercontent.com/docs/securesc/qcuv6cummtn6buhr3r2es7rfvmp540vi/qcv5lamitit30pnu24lj94l1tgqq2sgu/1631072175000/18124883206713037767/10158580143818279667Z/1MBrFIxKOW_pxbeAVshmuGp8SKakrMzkw?e=download [following]\n",
      "--2021-09-08 12:37:14--  https://doc-0k-as-docs.googleusercontent.com/docs/securesc/qcuv6cummtn6buhr3r2es7rfvmp540vi/qcv5lamitit30pnu24lj94l1tgqq2sgu/1631072175000/18124883206713037767/10158580143818279667Z/1MBrFIxKOW_pxbeAVshmuGp8SKakrMzkw?e=download\n",
      "Resolving doc-0k-as-docs.googleusercontent.com (doc-0k-as-docs.googleusercontent.com)... 142.250.199.97, 2404:6800:4004:823::2001\n",
      "Connecting to doc-0k-as-docs.googleusercontent.com (doc-0k-as-docs.googleusercontent.com)|142.250.199.97|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://docs.google.com/nonceSigner?nonce=0se0a7acbpm8c&continue=https://doc-0k-as-docs.googleusercontent.com/docs/securesc/qcuv6cummtn6buhr3r2es7rfvmp540vi/qcv5lamitit30pnu24lj94l1tgqq2sgu/1631072175000/18124883206713037767/10158580143818279667Z/1MBrFIxKOW_pxbeAVshmuGp8SKakrMzkw?e%3Ddownload&hash=6t6ek1lqa7opnmd42m3kij1u8spcpf8s [following]\n",
      "--2021-09-08 12:37:14--  https://docs.google.com/nonceSigner?nonce=0se0a7acbpm8c&continue=https://doc-0k-as-docs.googleusercontent.com/docs/securesc/qcuv6cummtn6buhr3r2es7rfvmp540vi/qcv5lamitit30pnu24lj94l1tgqq2sgu/1631072175000/18124883206713037767/10158580143818279667Z/1MBrFIxKOW_pxbeAVshmuGp8SKakrMzkw?e%3Ddownload&hash=6t6ek1lqa7opnmd42m3kij1u8spcpf8s\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.175.78|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://doc-0k-as-docs.googleusercontent.com/docs/securesc/qcuv6cummtn6buhr3r2es7rfvmp540vi/qcv5lamitit30pnu24lj94l1tgqq2sgu/1631072175000/18124883206713037767/10158580143818279667Z/1MBrFIxKOW_pxbeAVshmuGp8SKakrMzkw?e=download&nonce=0se0a7acbpm8c&user=10158580143818279667Z&hash=do1k07jq8il8kh1140c5jvdsrmg1a30v [following]\n",
      "--2021-09-08 12:37:15--  https://doc-0k-as-docs.googleusercontent.com/docs/securesc/qcuv6cummtn6buhr3r2es7rfvmp540vi/qcv5lamitit30pnu24lj94l1tgqq2sgu/1631072175000/18124883206713037767/10158580143818279667Z/1MBrFIxKOW_pxbeAVshmuGp8SKakrMzkw?e=download&nonce=0se0a7acbpm8c&user=10158580143818279667Z&hash=do1k07jq8il8kh1140c5jvdsrmg1a30v\n",
      "Connecting to doc-0k-as-docs.googleusercontent.com (doc-0k-as-docs.googleusercontent.com)|142.250.199.97|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘pretrained.zip’\n",
      "\n",
      "pretrained.zip          [              <=>   ] 932.93M  61.9MB/s    in 17s     \n",
      "\n",
      "2021-09-08 12:37:32 (56.1 MB/s) - ‘pretrained.zip’ saved [978251250]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write a text file about the overall information\n",
    "write_info(final_directory, n_subjects, window_length_min, n_samples_run, n_units, n_epochs, batch_size, lr_init, lr_min, lr_beginanneal, lr_drate, beta_max, beta_lr, optimizer_algorithm)\n",
    "\n",
    "# Load input data of DNN\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test \\\n",
    " = load_data(directory_open, subject_list, window_length_min, n_samples_run, selected_units)\n",
    "\n",
    "# Number of mini-batches in an epoch\n",
    "n_batches = int(np.ceil(np.size(x_train,0)/batch_size))\n",
    "\n",
    "if ~os.path.isdir('./pretrained'):\n",
    "    # Download pretrained model (~1GB) via Google drive\n",
    "    !wget --load-cookies ~/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies ~/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1MBrFIxKOW_pxbeAVshmuGp8SKakrMzkw' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1MBrFIxKOW_pxbeAVshmuGp8SKakrMzkw\" -O pretrained.zip && rm -rf ~/cookies.txt\n",
    "    with zipfile.ZipFile(\"./pretrained.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, build a DNN with same structure as pretrained model (to restore pretrained Tensorflow variables easily)\n",
    "X, Y, dnn, dnn_bn, dnn_bn_act, unit_vec_split_pre, n_units_pre, is_train = init_pretrained_dnn(n_units, n_hid_layers)\n",
    "\n",
    "# Initialize operations and variables related to training\n",
    "cost, optimizer, Lr, Beta, Gamma, N_batches = init_cost_optimizer(n_units_pre, unit_vec_split_pre, n_hid_layers, n_sp_layers, dnn, Y)\n",
    "\n",
    "# Create an instance to restore variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Next, substitute the last layer and its related variables with a new one for our data\n",
    "Y, dnn, dnn_bn, dnn_bn_act, unit_vec_split = init_dnn_last_layer(n_units, n_hid_layers, dnn, dnn_bn, dnn_bn_act)\n",
    "\n",
    "with tf.name_scope(\"new_train_test\"):\n",
    "    # Initialize training operations and variables again for the updated DNN structure\n",
    "    cost, optimizer, Lr, Beta, Gamma, N_batches = init_cost_optimizer(n_units, unit_vec_split, n_hid_layers, n_sp_layers, dnn, Y)\n",
    "\n",
    "    # Calculate an average error depending on how frequently classified correctly\n",
    "    predict_ans = tf.argmax(dnn_bn_act[-1], 1)\n",
    "    correct_ans = tf.argmax(Y, 1)\n",
    "    error = 1-tf.reduce_mean(tf.cast(tf.equal(predict_ans,correct_ans), tf.float32))\n",
    "\n",
    "# Create lists and arrays to store results obtained during training\n",
    "lr, hsp, beta, beta_vec, plot_hsp, plot_beta, plot_lr, plot_tot_cost, plot_train_err, plot_val_err, plot_test_err \\\n",
    " = init_other_variables(n_subjects, lr_init, n_units, n_sp_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./pretrained/result_model.ckpt-2000\n",
      "<epoch 020> tr err: 0.000 /vd err: 0.207 /ts err: 0.153 /HSP: ['0.73', '0.95']\n",
      "<epoch 040> tr err: 0.000 /vd err: 0.137 /ts err: 0.081 /HSP: ['0.80', '0.95']\n",
      "<epoch 060> tr err: 0.000 /vd err: 0.116 /ts err: 0.070 /HSP: ['0.80', '0.95']\n",
      "<epoch 080> tr err: 0.000 /vd err: 0.112 /ts err: 0.060 /HSP: ['0.80', '0.95']\n",
      "<epoch 100> tr err: 0.000 /vd err: 0.105 /ts err: 0.056 /HSP: ['0.80', '0.95']\n",
      "<epoch 120> tr err: 0.000 /vd err: 0.105 /ts err: 0.054 /HSP: ['0.80', '0.95']\n",
      "<epoch 140> tr err: 0.000 /vd err: 0.104 /ts err: 0.054 /HSP: ['0.80', '0.95']\n",
      "<epoch 160> tr err: 0.000 /vd err: 0.104 /ts err: 0.054 /HSP: ['0.80', '0.95']\n",
      "<epoch 180> tr err: 0.000 /vd err: 0.100 /ts err: 0.058 /HSP: ['0.80', '0.95']\n",
      "<epoch 200> tr err: 0.000 /vd err: 0.098 /ts err: 0.056 /HSP: ['0.80', '0.95']\n",
      "******************************* Final results **********************************\n",
      "Training finished! It ran for 130.9 mins \n",
      "\n",
      "\n",
      "=> Final validation & test error rate = 0.098 & 0.056\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################# Start training #################################################\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Tensorflow's operations and tensors can be executed/evaluated in the session\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=False, gpu_options=tf.GPUOptions(allow_growth=True))) as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    # Restore variables of pretrained model\n",
    "    saver.restore(sess, \"./pretrained/result_model.ckpt-2000\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Shuffle sample order before starting an epoch\n",
    "        tr_ids_shuff = np.arange(np.size(x_train,0), dtype=int)\n",
    "        np.random.shuffle(tr_ids_shuff)\n",
    "\n",
    "        # Learning rate annealing\n",
    "        if lr_beginanneal==0:\n",
    "            lr = lr*1.0\n",
    "        elif epoch+1 >lr_beginanneal:\n",
    "            lr = max(lr_min, (-lr_drate*(epoch+1) +(1+lr_drate*lr_beginanneal))*lr)\n",
    "\n",
    "        tot_cost_epoch = np.float32(0.0)\n",
    "\n",
    "        # Mini-batch based training\n",
    "        for batch in range(n_batches):\n",
    "            if batch<(n_batches-1):\n",
    "                x_batch, y_batch = x_train[tr_ids_shuff[batch*batch_size:(batch+1)*batch_size],:], \\\n",
    "                                   y_train[tr_ids_shuff[batch*batch_size:(batch+1)*batch_size],:]\n",
    "            else:\n",
    "                # for the last batch\n",
    "                x_batch, y_batch = x_train[tr_ids_shuff[batch*batch_size:],:], \\\n",
    "                                   y_train[tr_ids_shuff[batch*batch_size:],:]\n",
    "\n",
    "            tot_cost_batch,_ = sess.run([cost, optimizer],\\\n",
    "                               {Lr:lr, Beta:beta_vec, Gamma:gamma, N_batches:n_batches, is_train:True, X:x_batch, Y:y_batch})\n",
    "\n",
    "            tot_cost_epoch += np.float32(tot_cost_batch)\n",
    "\n",
    "            # Weight sparsity control\n",
    "            for layer in range(n_sp_layers):\n",
    "                w_layer = tf.get_default_graph().get_tensor_by_name(os.path.split(dnn[layer].name)[0]+'/kernel:0')\n",
    "                [hsp[layer], beta[layer]] = Hoyers_sparsity_control(beta_lr[layer], sess.run(w_layer), beta[layer], beta_max[layer], tg_sp_set[layer])\n",
    "\n",
    "            # Vectorized all beta values across layers (e.g., (2, 2000) -> (4000,))\n",
    "            beta_vec = [item for sublist in beta for item in sublist]\n",
    "\n",
    "        # Save the results to plot at a certain epoch\n",
    "        plot_lr = np.hstack([plot_lr, [lr]])\n",
    "        plot_tot_cost = np.hstack([plot_tot_cost, [tot_cost_epoch]])\n",
    "        plot_hsp = [np.vstack([plot_hsp[layer], [np.transpose(hsp[layer])]]) for layer in range(n_sp_layers)]\n",
    "        plot_beta = [np.vstack([plot_beta[layer], [np.transpose(beta[layer])]]) for layer in range(n_sp_layers)]\n",
    "\n",
    "        # Get the classification performance\n",
    "        plot_train_err, plot_val_err, plot_test_err \\\n",
    "            = get_err(sess, hsp, epoch, epoch_step_show, n_subjects, n_samples_run, x_train, y_train, x_valid, y_valid, x_test, y_test, \\\n",
    "                             plot_train_err, plot_val_err, plot_test_err,error, X, Y, is_train)\n",
    "\n",
    "\n",
    "    ################################################# Finish training #################################################\n",
    "\n",
    "    # Discard the zeroth element since it is empty\n",
    "    plot_lr = plot_lr[1:]\n",
    "    plot_tot_cost = plot_tot_cost[1:]\n",
    "    plot_train_err = plot_train_err[:,1:]\n",
    "    plot_val_err = plot_val_err[:,1:]\n",
    "    plot_test_err = plot_test_err[:,1:]\n",
    "    for layer in range(n_sp_layers):\n",
    "        plot_beta[layer] = plot_beta[layer][1:]\n",
    "        plot_hsp[layer] = plot_hsp[layer][1:]\n",
    "\n",
    "    # Plot results and save mat files\n",
    "    plot_save_results(sess, final_directory, tg_sp_set, plot_lr, plot_tot_cost, plot_train_err, plot_val_err, plot_test_err, plot_beta, plot_hsp, lr_init, beta_max, dnn, n_units, n_hid_layers, n_sp_layers)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    # Save the final performance and running time in a text file\n",
    "    print('******************************* Final results **********************************')\n",
    "    print('Training finished! It ran for %.1f mins \\n\\n'%((end_time-start_time)/60))\n",
    "    print('=> Final validation & test error rate = {:.3f}'.format(np.mean(plot_val_err[:,-1])),'& {:.3f}'.format(np.mean(plot_test_err[:,-1])))\n",
    "\n",
    "    f = open(final_directory+'/final_info.txt','w')\n",
    "    f.write('Training finished! It ran for %.1f mins \\n\\n'%((end_time-start_time)/60))\n",
    "    f.write('=> Final validation & test error rate = {:.3f}'.format(np.mean(plot_val_err[:,-1]))+' & {:.3f}'.format(np.mean(plot_test_err[:,-1])))\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
